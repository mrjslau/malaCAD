{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read annotations.csv and candidates.csv // paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../dataset/'\n",
    "TRAIN_IMG_DIR = '../img/train/'\n",
    "TEST_IMG_DIR = '../img/test/'\n",
    "IMG_PREF = 'image_'\n",
    "IMG_EXT = '.jpg'\n",
    "\n",
    "annot_df = pd.read_csv(data_path + 'CSVFILES/annotations.csv')\n",
    "cand_df = pd.read_csv(data_path + 'CSVFILES/candidates.csv')\n",
    "# candv2_df = pd.read_csv('./dataset/CSVFILES/candidates_V2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotations.csv\n",
    "##### Samples:\n",
    "<b> 1186 </b> samples\n",
    "\n",
    "### candidates.csv\n",
    "##### Samples\n",
    "<b> 1351 </b> TP in candidates.csv \n",
    "|| candidates have <b> 551065 </b> samples\n",
    "\n",
    "\n",
    "<b> 1557 </b> TP in candidates_V2.csv \n",
    "|| candidates_V2 have <b> 754975 </b> samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process CT scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT(object):\n",
    "    def __init__(self, filename = None, coordinates = None):\n",
    "        self.filename = filename\n",
    "        self.coordinates = coordinates\n",
    "        self.itk_img = None\n",
    "        self.img_arr = None\n",
    "        \n",
    "    # Read .mhd/.raw with SimpleITK\n",
    "    def read_mhd(self):\n",
    "        path = self.get_mhd_path()\n",
    "        self.itk_img = sitk.ReadImage(path[0])\n",
    "        self.img_arr = sitk.GetArrayFromImage(self.itk_img)\n",
    "         \n",
    "    def get_mhd_path(self):\n",
    "        return glob.glob(data_path + '*/' + self.filename + '.mhd')\n",
    "    \n",
    "    # Get voxel coordinates\n",
    "    def get_voxel(self):\n",
    "        origin = self.get_origin()\n",
    "        spacing = self.get_spacing()\n",
    "        coordinates = self.get_coordinates()\n",
    "        return tuple([np.absolute(coordinates[i] - origin[i]) / spacing[i] for i in range(3)])\n",
    "    \n",
    "    def get_coordinates(self):\n",
    "        return self.coordinates\n",
    "    \n",
    "    def get_origin(self):\n",
    "        return self.itk_img.GetOrigin()\n",
    "    \n",
    "    def get_spacing(self):\n",
    "        return self.itk_img.GetSpacing()\n",
    "    \n",
    "    def get_itk_img(self):\n",
    "        return self.itk_img\n",
    "    \n",
    "    def get_image_array(self):\n",
    "        return self.img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Spacing </b><br> Pixel spacing is measured between the pixel centers <br> and can be different along each dimension\n",
    "\n",
    "<b> Origin </b><br> The image origin is associated with the coordinates <br> of the first pixel in the image\n",
    "\n",
    "<b> Voxel </b><br> Voxel coordinates ... <br> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ind = cand_df[cand_df['class'] == 1].index # 1351 positives\n",
    "neg_ind = cand_df[cand_df['class'] == 0].index # 549714 negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersample negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8106 entries, 13 to 9679\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   seriesuid  8106 non-null   object \n",
      " 1   coordX     8106 non-null   float64\n",
      " 2   coordY     8106 non-null   float64\n",
      " 3   coordZ     8106 non-null   float64\n",
      " 4   class      8106 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 380.0+ KB\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Take negative samples only 5 times the size of positive samples\n",
    "neg_ind = np.random.choice(neg_ind, len(pos_ind) * 5 , replace = False)\n",
    "selected_cand_df = cand_df.iloc[list(pos_ind) + list(neg_ind)]\n",
    "\n",
    "# 1351 positive samples (16.66%)\n",
    "# 6755 negative samples (83.33%)\n",
    "selected_cand_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to train test val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = selected_cand_df.iloc[:, :-1]\n",
    "y = selected_cand_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Serialize and save data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('./pickle/traindata')\n",
    "X_val.to_pickle('./pickle/valdata')\n",
    "X_test.to_pickle('./pickle/testdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Total examples:  5187\n",
      "Total positives:  845\n",
      "16.29 %\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING DATA')\n",
    "print('Total examples: ', len(y_train))\n",
    "print('Total positives: ', y_train.sum())\n",
    "print(format((y_train.sum() / len(y_train)) * 100, '.2f'), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### augment data set\n",
    "\n",
    "<b> Add all the true positive cases two more times </b>\n",
    "\n",
    "* DF use .set_index\n",
    "* Series use .reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Augment X set ----------------------\n",
    "# 1 pass -----------------------------\n",
    "# take positives\n",
    "temp_df = X_train[y_train == 1]\n",
    "# reindex positives\n",
    "temp_df = temp_df.set_index(temp_df.index + 1000000)\n",
    "# add new positives\n",
    "X_train_aug = X_train.append(temp_df)\n",
    "\n",
    "# 2 pass -----------------------------\n",
    "temp_df = X_train[y_train == 1]\n",
    "temp_df = temp_df.set_index(temp_df.index + 2000000)\n",
    "X_train_aug = X_train_aug.append(temp_df)\n",
    "\n",
    "\n",
    "# Augment y set ----------------------\n",
    "# 1 pass -----------------------------\n",
    "# take positives\n",
    "temp_df = X_train[y_train == 1]\n",
    "# reindex positives\n",
    "temp_df = y_train.reindex(temp_df.index + 1000000)\n",
    "# set all series values to 1\n",
    "temp_df.loc[:] = 1\n",
    "# add new positives\n",
    "y_train_aug = y_train.append(temp_df)\n",
    "\n",
    "# 2 pass -----------------------------\n",
    "temp_df = X_train[y_train == 1]\n",
    "temp_df = y_train.reindex(temp_df.index + 2000000)\n",
    "temp_df.loc[:] = 1\n",
    "y_train_aug = y_train_aug.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Total examples:  6877\n",
      "Total positives:  2535.0\n",
      "36.86 %\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING DATA')\n",
    "print('Total examples: ', len(y_train_aug))\n",
    "print('Total positives: ', y_train_aug.sum())\n",
    "print(format((y_train_aug.sum() / len(y_train_aug)) * 100, '.2f'), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2b66c4d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imageio import imread\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "img = imread(TRAIN_IMG_DIR + IMG_PREF + str(30517) + IMG_EXT)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create h5py file with images and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('./h5py/malaCADtrain.hdf5', 'w')\n",
    "\n",
    "# Create classes dataset\n",
    "f.create_dataset('train_labels', shape = (len(y_train_new),))\n",
    "f['train_labels'][...] = list(y_train_new)\n",
    "\n",
    "# Create images dataset\n",
    "f.create_dataset('train_img', shape = (len(X_train_new), 50, 50))\n",
    "index = 0\n",
    "for col in X_train_new.transpose():\n",
    "    image = PIL.Image.open(TRAIN_IMG_DIR + IMG_PREF + str(col) + IMG_EXT)\n",
    "    # print(str(index) + ' - ' + str(col) + ' - ' + str(image.size))\n",
    "    f['train_img'][index, ...] = image\n",
    "    index += 1\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('./h5py/malaCADtrain.hdf5', 'r')\n",
    "\n",
    "input_train = f['train_img'] [...]\n",
    "label_train = f['train_labels'][...]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('./h5py/malaCADtest.hdf5', 'w')\n",
    "\n",
    "# Create classes dataset\n",
    "f.create_dataset('test_labels', shape = (len(y_test),))\n",
    "f['test_labels'][...] = list(y_test)\n",
    "\n",
    "# Create images dataset\n",
    "f.create_dataset('test_img', shape = (len(X_test), 50, 50))\n",
    "index = 0\n",
    "for col in X_test.transpose():\n",
    "    image = PIL.Image.open(TEST_IMG_DIR + IMG_PREF + str(col) + IMG_EXT)\n",
    "    # print(str(index) + ' - ' + str(col) + ' - ' + str(image.size))\n",
    "    f['test_img'][index, ...] = image\n",
    "    index += 1\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('./h5py/malaCADtest.hdf5', 'r')\n",
    "\n",
    "input_test = f['test_img'] [...]\n",
    "label_test = f['test_labels'][...]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create and config model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_width, img_height, img_num_channels = 50, 50, 1\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 2\n",
    "no_epochs = 5\n",
    "optimizer = Adam()\n",
    "validation_split = 0.2\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data\n",
    "input_train = input_train.reshape(len(input_train), img_width, img_height, img_num_channels)\n",
    "input_test = input_test.reshape(len(input_test), img_width, img_height, img_num_channels)\n",
    "\n",
    "input_shape = (img_width, img_height, img_num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 247808)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               31719552  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 31,812,482\n",
      "Trainable params: 31,812,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape = input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5501 samples, validate on 1376 samples\n",
      "Epoch 1/5\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 31.2617 - accuracy: 0.7853 - val_loss: 1.3232 - val_accuracy: 0.3358\n",
      "Epoch 2/5\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 0.4346 - accuracy: 0.8355 - val_loss: 1.0953 - val_accuracy: 0.4608\n",
      "Epoch 3/5\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.3490 - accuracy: 0.8671 - val_loss: 1.3542 - val_accuracy: 0.3118\n",
      "Epoch 4/5\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.3121 - accuracy: 0.8740 - val_loss: 1.0675 - val_accuracy: 0.4804\n",
      "Epoch 5/5\n",
      "5501/5501 [==============================] - 63s 11ms/sample - loss: 0.2534 - accuracy: 0.9044 - val_loss: 1.5436 - val_accuracy: 0.4092\n"
     ]
    }
   ],
   "source": [
    "# Fit data to model\n",
    "history = model.fit(input_train, label_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.42157794408380767 / Test accuracy: 0.8569667339324951\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(input_test, label_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1269   71]\n",
      " [ 161  121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.95      0.92      1340\n",
      "         1.0       0.63      0.43      0.51       282\n",
      "\n",
      "    accuracy                           0.86      1622\n",
      "   macro avg       0.76      0.69      0.71      1622\n",
      "weighted avg       0.84      0.86      0.85      1622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(label_test, preds))\n",
    "print(classification_report(label_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2nd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5501 samples, validate on 1376 samples\n",
      "Epoch 1/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.1803 - accuracy: 0.9318 - val_loss: 1.4949 - val_accuracy: 0.5581\n",
      "Epoch 2/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.1399 - accuracy: 0.9484 - val_loss: 1.9399 - val_accuracy: 0.5603\n",
      "Epoch 3/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 0.0765 - accuracy: 0.9738 - val_loss: 2.5228 - val_accuracy: 0.5683\n",
      "Epoch 4/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.0416 - accuracy: 0.9869 - val_loss: 4.0340 - val_accuracy: 0.5007\n",
      "Epoch 5/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.0346 - accuracy: 0.9895 - val_loss: 3.5961 - val_accuracy: 0.5472\n",
      "Epoch 6/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 0.0524 - accuracy: 0.9805 - val_loss: 5.0075 - val_accuracy: 0.5131\n",
      "Epoch 7/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 0.0440 - accuracy: 0.9856 - val_loss: 4.6940 - val_accuracy: 0.5189\n",
      "Epoch 8/20\n",
      "5501/5501 [==============================] - 63s 11ms/sample - loss: 0.0254 - accuracy: 0.9913 - val_loss: 4.1228 - val_accuracy: 0.5501\n",
      "Epoch 9/20\n",
      "5501/5501 [==============================] - 63s 11ms/sample - loss: 0.0093 - accuracy: 0.9975 - val_loss: 6.7102 - val_accuracy: 0.4811\n",
      "Epoch 10/20\n",
      "5501/5501 [==============================] - 63s 11ms/sample - loss: 0.0483 - accuracy: 0.9873 - val_loss: 4.0913 - val_accuracy: 0.5291\n",
      "Epoch 11/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 0.0284 - accuracy: 0.9940 - val_loss: 3.9422 - val_accuracy: 0.5683\n",
      "Epoch 12/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.0138 - accuracy: 0.9964 - val_loss: 5.6045 - val_accuracy: 0.5247\n",
      "Epoch 13/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 0.0140 - accuracy: 0.9962 - val_loss: 6.2818 - val_accuracy: 0.5153\n",
      "Epoch 14/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 0.0024 - accuracy: 0.9996 - val_loss: 6.3180 - val_accuracy: 0.5501\n",
      "Epoch 15/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 7.9869e-04 - accuracy: 1.0000 - val_loss: 6.2994 - val_accuracy: 0.5487\n",
      "Epoch 16/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 3.4419e-04 - accuracy: 1.0000 - val_loss: 6.6540 - val_accuracy: 0.5509\n",
      "Epoch 17/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 2.3972e-04 - accuracy: 1.0000 - val_loss: 6.8977 - val_accuracy: 0.5501\n",
      "Epoch 18/20\n",
      "5501/5501 [==============================] - 61s 11ms/sample - loss: 1.8335e-04 - accuracy: 1.0000 - val_loss: 7.0528 - val_accuracy: 0.5523\n",
      "Epoch 19/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 1.4535e-04 - accuracy: 1.0000 - val_loss: 7.2510 - val_accuracy: 0.5509\n",
      "Epoch 20/20\n",
      "5501/5501 [==============================] - 62s 11ms/sample - loss: 1.1996e-04 - accuracy: 1.0000 - val_loss: 7.3908 - val_accuracy: 0.5487\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_width, img_height, img_num_channels = 50, 50, 1\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 2\n",
    "no_epochs = 20\n",
    "optimizer = Adam()\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(input_train, label_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.781258276152699 / Test accuracy: 0.8520345091819763\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(input_test, label_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1227  113]\n",
      " [ 127  155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.92      0.91      1340\n",
      "         1.0       0.58      0.55      0.56       282\n",
      "\n",
      "    accuracy                           0.85      1622\n",
      "   macro avg       0.74      0.73      0.74      1622\n",
      "weighted avg       0.85      0.85      0.85      1622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(input_test)\n",
    "print(confusion_matrix(label_test, preds))\n",
    "print(classification_report(label_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd pass - less negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('./h5py/malaCADtrain2nd.hdf5', 'w')\n",
    "\n",
    "# Create classes dataset\n",
    "f.create_dataset('train_labels', shape = (len(y_train_new),))\n",
    "f['train_labels'][...] = list(y_train_new)\n",
    "\n",
    "# Create images dataset\n",
    "f.create_dataset('train_img', shape = (len(X_train_new), 50, 50))\n",
    "index = 0\n",
    "for col in X_train_new.transpose():\n",
    "    image = PIL.Image.open(TRAIN_IMG_DIR + IMG_PREF + str(col) + IMG_EXT)\n",
    "    # print(str(index) + ' - ' + str(col) + ' - ' + str(image.size))\n",
    "    f['train_img'][index, ...] = image\n",
    "    index += 1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "f = h5py.File('./h5py/malaCADtrain2nd.hdf5', 'r')\n",
    "input_train = f['train_img'] [...]\n",
    "label_train = f['train_labels'][...]\n",
    "f.close()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "f = h5py.File('./h5py/malaCADtest2nd.hdf5', 'w')\n",
    "\n",
    "# Create classes dataset\n",
    "f.create_dataset('test_labels', shape = (len(y_test),))\n",
    "f['test_labels'][...] = list(y_test)\n",
    "\n",
    "# Create images dataset\n",
    "f.create_dataset('test_img', shape = (len(X_test), 50, 50))\n",
    "index = 0\n",
    "for col in X_test.transpose():\n",
    "    image = PIL.Image.open(TEST_IMG_DIR + IMG_PREF + str(col) + IMG_EXT)\n",
    "    # print(str(index) + ' - ' + str(col) + ' - ' + str(image.size))\n",
    "    f['test_img'][index, ...] = image\n",
    "    index += 1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "f = h5py.File('./h5py/malaCADtest2nd.hdf5', 'r')\n",
    "input_test = f['test_img'] [...]\n",
    "label_test = f['test_labels'][...]\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
