{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read annotations.csv and candidates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/'\n",
    "TRAINDATA_LABELS_FILE = 'traindatalabels.txt'\n",
    "\n",
    "annot_df = pd.read_csv(data_path + 'CSVFILES/annotations.csv')\n",
    "cand_df = pd.read_csv(data_path + 'CSVFILES/candidates.csv')\n",
    "# candv2_df = pd.read_csv('./dataset/CSVFILES/candidates_V2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations\n",
    "##### Columns:\n",
    "* seriesuid\n",
    "* coordX\n",
    "* coordY\n",
    "* coordZ\n",
    "* diameter_mm\n",
    "\n",
    "##### Samples:\n",
    "<b> 1186 </b> samples\n",
    "\n",
    "----\n",
    "----\n",
    "\n",
    "### Candidates\n",
    "##### Columns:\n",
    "* seriesuid\n",
    "* coordX\n",
    "* coordY\n",
    "* coordZ\n",
    "* class (0 or 1; 1 => cancer)\n",
    "\n",
    "#### Candidates vs Candidates_V2\n",
    "<b> 1351 </b> TP in candidates.csv \n",
    "|| candidates have <b> 551065 </b> samples\n",
    "\n",
    "\n",
    "<b> 1557 </b> TP in candidates_V2.csv \n",
    "|| candidates_V2 have <b> 754975 </b> samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process CT scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT(object):\n",
    "    def __init__(self, filename = None, coordinates = None):\n",
    "        self.filename = filename\n",
    "        self.coordinates = coordinates\n",
    "        self.itk_img = None\n",
    "        self.img_arr = None\n",
    "        \n",
    "    # Read .mhd/.raw with SimpleITK\n",
    "    def read_mhd(self):\n",
    "        path = self.get_mhd_path()\n",
    "        self.itk_img = sitk.ReadImage(path[0])\n",
    "        self.img_arr = sitk.GetArrayFromImage(self.itk_img)\n",
    "         \n",
    "    def get_mhd_path(self):\n",
    "        return glob.glob(data_path + '*/' + self.filename + '.mhd')\n",
    "    \n",
    "    # Get voxel coordinates\n",
    "    def get_voxel(self):\n",
    "        origin = self.get_origin()\n",
    "        spacing = self.get_spacing()\n",
    "        coordinates = self.get_coordinates()\n",
    "        return tuple([np.absolute(coordinates[i] - origin[i]) / spacing[i] for i in range(3)])\n",
    "    \n",
    "    def get_coordinates(self):\n",
    "        return self.coordinates\n",
    "    \n",
    "    def get_origin(self):\n",
    "        return self.itk_img.GetOrigin()\n",
    "    \n",
    "    def get_spacing(self):\n",
    "        return self.itk_img.GetSpacing()\n",
    "    \n",
    "    def get_itk_img(self):\n",
    "        return self.itk_img\n",
    "    \n",
    "    def get_image_array(self):\n",
    "        return self.img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Spacing </b><br> Pixel spacing is measured between the pixel centers <br> and can be different along each dimension\n",
    "\n",
    "<b> Origin </b><br> The image origin is associated with the coordinates <br> of the first pixel in the image\n",
    "\n",
    "<b> Voxel </b><br> Voxel coordinates ... <br> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ind = cand_df[cand_df['class'] == 1].index # 1351 positives\n",
    "neg_ind = cand_df[cand_df['class'] == 0].index # 549714 negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Test </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = np.asarray(cand_df.iloc[neg_ind[600]])[0]\n",
    "# position = np.asarray(cand_df.iloc[neg_ind[600]][1:-1])\n",
    "\n",
    "# scan = CT(name, position)\n",
    "# scan.read_mhd()\n",
    "# print(scan.get_voxel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersample negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ind = np.random.choice(neg_ind, len(pos_ind) * 5 , replace = False)\n",
    "selected_cand_df = cand_df.iloc[list(pos_ind) + list(neg_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8106 entries, 13 to 411819\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   seriesuid  8106 non-null   object \n",
      " 1   coordX     8106 non-null   float64\n",
      " 2   coordY     8106 non-null   float64\n",
      " 3   coordZ     8106 non-null   float64\n",
      " 4   class      8106 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 380.0+ KB\n"
     ]
    }
   ],
   "source": [
    "selected_cand_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to train test val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = selected_cand_df.iloc[:, :-1]\n",
    "y = selected_cand_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Serialize and save data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('traindata')\n",
    "X_val.to_pickle('valdata')\n",
    "X_test.to_pickle('testdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Total examples:  5187\n",
      "Total positives:  880\n",
      "16.97 %\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING DATA')\n",
    "print('Total examples: ', len(y_train))\n",
    "print('Total positives: ', y_train.sum())\n",
    "print(format((y_train.sum() / len(y_train)) * 100, '.2f'), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### augment data set\n",
    "\n",
    "<b> Add all the true positive cases two more times </b>\n",
    "\n",
    "* DF use .set_index\n",
    "* Series use .reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Augment X set ----------------------\n",
    "# 1 pass\n",
    "# take positives\n",
    "temp_df = X_train[y_train == 1]\n",
    "# reindex positives\n",
    "temp_df = temp_df.set_index(temp_df.index + 1000000)\n",
    "# add new positives\n",
    "X_train_new = X_train.append(temp_df)\n",
    "\n",
    "# 2 pass\n",
    "temp_df = X_train[y_train == 1]\n",
    "temp_df = temp_df.set_index(temp_df.index + 2000000)\n",
    "X_train_new = X_train_new.append(temp_df)\n",
    "\n",
    "\n",
    "# Augment y set ----------------------\n",
    "# 1 pass\n",
    "# take positives\n",
    "temp_df = X_train[y_train == 1]\n",
    "# reindex positives\n",
    "temp_df = y_train.reindex(temp_df.index + 1000000)\n",
    "# set all series values to 1\n",
    "temp_df.loc[:] = 1\n",
    "# add new positives\n",
    "y_train_new = y_train.append(temp_df)\n",
    "\n",
    "# 2 pass\n",
    "temp_df = X_train[y_train == 1]\n",
    "temp_df = y_train.reindex(temp_df.index + 2000000)\n",
    "temp_df.loc[:] = 1\n",
    "y_train_new = y_train_new.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "class PreProcess(object):\n",
    "    def __init__(self, image = None):\n",
    "        self.image = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a4ba8ca90>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imageio import imread\n",
    "\n",
    "img_dir = './img/train/'\n",
    "plt.figure(figsize = (5, 5))\n",
    "img = imread(img_dir + 'image_' + str(30517) + '.jpg')\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "### load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = X_train_new.index.to_series().apply(lambda x:\n",
    "                                                      './img/train/image_' + str(x) + '.jpg')\n",
    "\n",
    "                                            # bytes string literal & integer\n",
    "traindata = np.zeros(filenames.size, dtype = [('var1', 'S36'), ('var2', int)])\n",
    "traindata['var1'] = train_filenames.values.astype(str)\n",
    "traindata['var2'] = y_train_new.values.astype(int)\n",
    "\n",
    "np.savetxt(TRAINDATA_LABELS_FILE, traindata, fmt=\"%10s %d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build h5py dataset    TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# f = h5py.File('dataset.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_hdf5_image_dataset(dataset_file, image_shape=(50, 50),\n",
    "#                         mode='file', output_path='traindataset.h5',\n",
    "#                         categorical_labels=True, normalize=True)\n",
    "for file, label in traindata:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load h5py data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5_train = h5py.File('traindataset.h5', 'r')\n",
    "X_train_images = h5_train['X']\n",
    "y_train_labels = h5_train['Y']\n",
    "\n",
    "h5_val = h5py.File('traindataset.h5', 'r')\n",
    "X_val_images = h5_val['X']\n",
    "y_val_labels = h5_val['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'h5py._hl.dataset.Dataset'>, <class 'NoneType'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_9_input to have 4 dimensions, but got array with shape (5187, 50, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-412-85a502957034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train_images, y_train_labels, epochs = 2, \n\u001b[0;32m----> 2\u001b[0;31m           validation_data = (X_val_images, y_val_labels))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nod_model.tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_9_input to have 4 dimensions, but got array with shape (5187, 50, 50)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_images, y_train_labels, epochs = 2, \n",
    "          validation_data = (X_val_images, y_val_labels))\n",
    "\n",
    "model.save('nod_model.tf')\n",
    "\n",
    "#model.fit(X_train_images, Y_train_labels, n_epoch=100, shuffle=True, validation_set=(X_val_images, Y_val_labels),\n",
    "          #show_metric=True, batch_size=96,snapshot_epoch=True,run_id='nodule-classifier')\n",
    "\n",
    "# Save model when training is complete to a file\n",
    "#model.save(\"nodule-classifier.tfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
